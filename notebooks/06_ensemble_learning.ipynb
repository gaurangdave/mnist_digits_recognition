{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 7 - Ensemble Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* This notebook is created to execute and experiment with the exercise from Chapter 7 of \"Hands-on ML\" book. \n",
    "* The exercise details are as follows, \n",
    "    * Load the MNIST dataset (introduced in Chapter 3), and split it into a training set, a validation set, and a test set (e.g., use 50,000 instances for training, 10,000 for validation, and 10,000 for testing).\n",
    "    * Then train various classifiers, such as a random forest classifier, an extra-trees classifier, and an SVM classifier. \n",
    "    * Next, try to combine them into an ensemble that outperforms each individual classifier on the validation set, using soft or hard voting. \n",
    "    * Once you have found one, try it on the test set. How much better does it perform compared to the individual classifiers?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plan\n",
    "* After experimenting in previous we realized that the model with Augmented dataset performs a lot better than the default regular dataset. So thats what I want to use for this exercise. \n",
    "* The plan is as follows,\n",
    "    * Split the data into training set, validation set and test set. \n",
    "    * Augment the entire dataset (instead of just the training set as before) and save it as CSV for future use.     \n",
    "    * Train the following classifiers, (we've trained them in previous notebooks)\n",
    "        * Logistic Regression\n",
    "        * SVC\n",
    "        * Random Foreset\n",
    "        * KNN\n",
    "        * Extra Trees Classifier\n",
    "        * Gradient Boosting Classifier\n",
    "    * We'll just try `Hard Voting` for now since for `Soft Voting` we need probabilities and calculating those for `SVM` can be time consuming. \n",
    "    * For the first version I think I'll train them using default params and then may be try training them using best params I got in previous notebooks. \n",
    "    * Compare the performance against individual classifiers and best classifier in previous notebook\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import Binarizer, OneHotEncoder, MinMaxScaler, StandardScaler, FunctionTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import cross_val_score, cross_val_predict,GridSearchCV\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from scipy.stats import loguniform\n",
    "from pathlib import Path\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier, GradientBoostingClassifier, VotingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "\n",
    "from sklearn.metrics import ConfusionMatrixDisplay, f1_score, roc_auc_score, roc_curve, accuracy_score\n",
    "from sklearn.dummy import DummyClassifier\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.express as px\n",
    "import joblib\n",
    "import json\n",
    "import gdown\n",
    "import os\n",
    "import sys\n",
    "import tqdm\n",
    "import time\n",
    "import warnings\n",
    "from huggingface_hub import login, HfApi,list_repo_files, hf_hub_download\n",
    "import os\n",
    "import sys\n",
    "import dotenv\n",
    "from pathlib import Path\n",
    "import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Hugging Face"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Note: Environment variable`HF_TOKEN` is set and is the current active token independently from the token you've just configured.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logging in...\n",
      "Logged in as gaurangdave\n"
     ]
    }
   ],
   "source": [
    "dotenv.load_dotenv()\n",
    "HF_TOKEN = os.getenv(\"HF_TOKEN\")\n",
    "\n",
    "## login huggingface user\n",
    "if HF_TOKEN is None:\n",
    "    print(\"Please set the HF_TOKEN environment variable. This is you hugging face token\")\n",
    "else:\n",
    "    print(\"Logging in...\")\n",
    "    login(HF_TOKEN)\n",
    "    \n",
    "api = HfApi()\n",
    "user = api.whoami()\n",
    "user_name = user['name']\n",
    "print(f\"Logged in as {user_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download Models & Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Get the directory of the notebook\n",
    "## this is root directory of jupyterlab\n",
    "notebook_dir = os.getcwd()\n",
    "\n",
    "# from api.utils.common import augment_dataset,download_data_from_gdrive\n",
    "project_root = os.path.abspath(os.path.join(notebook_dir , \"mnist_digits_recognition\"))  \n",
    "model_root = os.path.join(project_root, \"models\")\n",
    "data_root = os.path.join(project_root, \"data\")\n",
    "\n",
    "## hugging face repo id\n",
    "model_name = \"mnist_digits_recognition\"\n",
    "repo_id = f\"{user_name}/{model_name}\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Models in gaurangdave/mnist_digits_recognition:\n",
      "Downloaded .gitattributes to /home/gaurangdave/jupyterlab/mnist_digits_recognition/models/.gitattributes\n",
      "Downloaded default_logistic_regression_probabilites.csv to /home/gaurangdave/jupyterlab/mnist_digits_recognition/models/default_logistic_regression_probabilites.csv\n",
      "Downloaded ensemble/extra_trees_model.pkl to /home/gaurangdave/jupyterlab/mnist_digits_recognition/models/ensemble/extra_trees_model.pkl\n",
      "Downloaded ensemble/gradient_boosting_model.pkl to /home/gaurangdave/jupyterlab/mnist_digits_recognition/models/ensemble/gradient_boosting_model.pkl\n",
      "Downloaded ensemble/knn_model.pkl to /home/gaurangdave/jupyterlab/mnist_digits_recognition/models/ensemble/knn_model.pkl\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e0e35ecd1e4a4d06b5a7650cfd01aed9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "logistic_regression_model.pkl:   0%|          | 0.00/109k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded ensemble/logistic_regression_model.pkl to /home/gaurangdave/jupyterlab/mnist_digits_recognition/models/ensemble/logistic_regression_model.pkl\n",
      "Downloaded ensemble/random_forest_model.pkl to /home/gaurangdave/jupyterlab/mnist_digits_recognition/models/ensemble/random_forest_model.pkl\n",
      "Downloaded ensemble/soft_voting_classifier_model.pkl to /home/gaurangdave/jupyterlab/mnist_digits_recognition/models/ensemble/soft_voting_classifier_model.pkl\n",
      "Downloaded ensemble/svc_model.pkl to /home/gaurangdave/jupyterlab/mnist_digits_recognition/models/ensemble/svc_model.pkl\n",
      "Downloaded ensemble/tuned_knn_model.pkl to /home/gaurangdave/jupyterlab/mnist_digits_recognition/models/ensemble/tuned_knn_model.pkl\n",
      "Downloaded ensemble/tuned_logistic_regression_model.pkl to /home/gaurangdave/jupyterlab/mnist_digits_recognition/models/ensemble/tuned_logistic_regression_model.pkl\n",
      "Downloaded ensemble/tuned_logistic_regression_v0.pkl to /home/gaurangdave/jupyterlab/mnist_digits_recognition/models/ensemble/tuned_logistic_regression_v0.pkl\n",
      "Downloaded ensemble/tuned_random_forest_model.pkl to /home/gaurangdave/jupyterlab/mnist_digits_recognition/models/ensemble/tuned_random_forest_model.pkl\n",
      "Downloaded ensemble/tuned_svc_model.pkl to /home/gaurangdave/jupyterlab/mnist_digits_recognition/models/ensemble/tuned_svc_model.pkl\n",
      "Downloaded ensemble/voting_classifier_model.pkl to /home/gaurangdave/jupyterlab/mnist_digits_recognition/models/ensemble/voting_classifier_model.pkl\n",
      "Downloaded knn_v0.joblib to /home/gaurangdave/jupyterlab/mnist_digits_recognition/models/knn_v0.joblib\n",
      "Downloaded knn_v1.joblib to /home/gaurangdave/jupyterlab/mnist_digits_recognition/models/knn_v1.joblib\n",
      "Downloaded logistic_regression_v0.joblib to /home/gaurangdave/jupyterlab/mnist_digits_recognition/models/logistic_regression_v0.joblib\n",
      "Downloaded logistic_regression_v1.joblib to /home/gaurangdave/jupyterlab/mnist_digits_recognition/models/logistic_regression_v1.joblib\n",
      "Downloaded logistic_regression_v1_cv_results.joblib to /home/gaurangdave/jupyterlab/mnist_digits_recognition/models/logistic_regression_v1_cv_results.joblib\n",
      "Downloaded logistic_regression_v2.joblib to /home/gaurangdave/jupyterlab/mnist_digits_recognition/models/logistic_regression_v2.joblib\n",
      "Downloaded mnist_models_metrics.csv to /home/gaurangdave/jupyterlab/mnist_digits_recognition/models/mnist_models_metrics.csv\n",
      "Downloaded random_forest_v0.joblib to /home/gaurangdave/jupyterlab/mnist_digits_recognition/models/random_forest_v0.joblib\n",
      "Downloaded random_forest_v1.joblib to /home/gaurangdave/jupyterlab/mnist_digits_recognition/models/random_forest_v1.joblib\n",
      "Downloaded svc_augmented_data_v1.joblib to /home/gaurangdave/jupyterlab/mnist_digits_recognition/models/svc_augmented_data_v1.joblib\n",
      "Downloaded svc_prod.joblib to /home/gaurangdave/jupyterlab/mnist_digits_recognition/models/svc_prod.joblib\n",
      "Downloaded svc_prod_v1.joblib to /home/gaurangdave/jupyterlab/mnist_digits_recognition/models/svc_prod_v1.joblib\n",
      "Downloaded svc_prod_v2.joblib to /home/gaurangdave/jupyterlab/mnist_digits_recognition/models/svc_prod_v2.joblib\n",
      "Downloaded svc_prod_v3.joblib to /home/gaurangdave/jupyterlab/mnist_digits_recognition/models/svc_prod_v3.joblib\n",
      "Downloaded svc_v0.joblib to /home/gaurangdave/jupyterlab/mnist_digits_recognition/models/svc_v0.joblib\n",
      "Downloaded svc_v1.joblib to /home/gaurangdave/jupyterlab/mnist_digits_recognition/models/svc_v1.joblib\n",
      "Downloaded svc_v2.joblib to /home/gaurangdave/jupyterlab/mnist_digits_recognition/models/svc_v2.joblib\n",
      "Downloaded svc_v3.joblib to /home/gaurangdave/jupyterlab/mnist_digits_recognition/models/svc_v3.joblib\n",
      "✅ All files downloaded to /home/gaurangdave/jupyterlab/mnist_digits_recognition/models\n"
     ]
    }
   ],
   "source": [
    "all_models = api.list_repo_files(repo_id, repo_type=\"model\")\n",
    "\n",
    "print(f\"Models in {repo_id}:\")\n",
    "for model in all_models:\n",
    "    ## download to tmp folder\n",
    "    file_path = hf_hub_download(repo_id=repo_id, filename=model, token=HF_TOKEN, local_dir=model_root, repo_type=\"model\")\n",
    "    ## move to model folder\n",
    "    os.rename(file_path, os.path.join(model_root, model))\n",
    "    print(f\"Downloaded {model} to {file_path}\")\n",
    "\n",
    "print(f\"✅ All files downloaded to {model_root}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datasets in gaurangdave/mnist_digits_recognition:\n",
      "Downloaded .gitattributes to /home/gaurangdave/jupyterlab/mnist_digits_recognition/data/.gitattributes\n",
      "Downloaded augmented_ensemle_learning_mnist_data.csv to /home/gaurangdave/jupyterlab/mnist_digits_recognition/data/augmented_ensemle_learning_mnist_data.csv\n",
      "Downloaded augmented_mnist_data.csv to /home/gaurangdave/jupyterlab/mnist_digits_recognition/data/augmented_mnist_data.csv\n",
      "Downloaded augmented_train_X.csv to /home/gaurangdave/jupyterlab/mnist_digits_recognition/data/augmented_train_X.csv\n",
      "Downloaded augmented_train_Y.csv to /home/gaurangdave/jupyterlab/mnist_digits_recognition/data/augmented_train_Y.csv\n",
      "Downloaded mnist_test_set.csv to /home/gaurangdave/jupyterlab/mnist_digits_recognition/data/mnist_test_set.csv\n",
      "Downloaded mnist_train_set.csv to /home/gaurangdave/jupyterlab/mnist_digits_recognition/data/mnist_train_set.csv\n",
      "Downloaded raw_mnist_data.csv to /home/gaurangdave/jupyterlab/mnist_digits_recognition/data/raw_mnist_data.csv\n",
      "Downloaded user_input.csv to /home/gaurangdave/jupyterlab/mnist_digits_recognition/data/user_input.csv\n",
      "Downloaded user_prediction_request.csv to /home/gaurangdave/jupyterlab/mnist_digits_recognition/data/user_prediction_request.csv\n",
      "✅ All files downloaded to /home/gaurangdave/jupyterlab/mnist_digits_recognition/models\n"
     ]
    }
   ],
   "source": [
    "all_data = api.list_repo_files(repo_id, repo_type=\"dataset\")\n",
    "    \n",
    "print(f\"Datasets in {repo_id}:\")\n",
    "for dataset in all_data:\n",
    "    ## download to tmp folder\n",
    "    file_path = hf_hub_download(repo_id=repo_id, filename=dataset, token=HF_TOKEN, local_dir=data_root, repo_type=\"dataset\")\n",
    "    ## move to model folder\n",
    "    os.rename(file_path, os.path.join(data_root, dataset))\n",
    "    print(f\"Downloaded {dataset} to {file_path}\")\n",
    "\n",
    "print(f\"✅ All files downloaded to {model_root}\")\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Custom Scripts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shift_image(image, direction):\n",
    "    \"\"\"\n",
    "    Shift an MNIST image in a given direction.\n",
    "\n",
    "    Args:\n",
    "        image (np.ndarray): 1D array of 784 pixels (28x28 image).\n",
    "        direction (str): One of 'up', 'down', 'left', 'right'.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: Shifted 1D image (flattened).\n",
    "    \"\"\"\n",
    "    # Reshape the flat image to 28x28\n",
    "    image_2d = image.reshape(28, 28)\n",
    "    # Create an empty 28x28 array filled with zeros\n",
    "    shifted = np.zeros_like(image_2d)\n",
    "\n",
    "    # Perform the shift based on the direction\n",
    "    if direction == \"up\":\n",
    "        shifted[:-1, :] = image_2d[1:, :]  # Shift rows up\n",
    "    elif direction == \"down\":\n",
    "        shifted[1:, :] = image_2d[:-1, :]  # Shift rows down\n",
    "    elif direction == \"left\":\n",
    "        shifted[:, :-1] = image_2d[:, 1:]  # Shift columns left\n",
    "    elif direction == \"right\":\n",
    "        shifted[:, 1:] = image_2d[:, :-1]  # Shift columns right\n",
    "\n",
    "    # Flatten the shifted image back to 1D\n",
    "    return shifted.flatten()\n",
    "\n",
    "\n",
    "def augment_dataset(X, y):\n",
    "    \"\"\"\n",
    "    Augment the MNIST dataset by creating shifted versions of each image.\n",
    "\n",
    "    Args:\n",
    "        X (pd.DataFrame): Dataset with features (flattened MNIST images).\n",
    "        y (pd.Series): Labels for the images.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame, pd.Series: Augmented dataset (features and labels).\n",
    "    \"\"\"\n",
    "    augmented_X = []\n",
    "    augmented_y = []\n",
    "\n",
    "    directions = [\"up\", \"down\", \"left\", \"right\"]\n",
    "    total_iterations = len(X)\n",
    "\n",
    "    # Iterate through each image and label\n",
    "    with tqdm.tqdm(total=total_iterations, desc=\"Augmenting Data\") as pbar:\n",
    "        for image, label in zip(X.values, y.values):\n",
    "            # Append the original image\n",
    "            augmented_X.append(image)\n",
    "            augmented_y.append(label)\n",
    "\n",
    "            # Create shifted images for all four directions\n",
    "            for direction in directions:\n",
    "                shifted_image = shift_image(image, direction)\n",
    "                augmented_X.append(shifted_image)\n",
    "                augmented_y.append(label)  # Same label for shifted image\n",
    "            pbar.update(1)\n",
    "\n",
    "    print(\"converting lists to DataFrame and Series\")\n",
    "    # Convert lists to DataFrame and Series\n",
    "    augmented_X = pd.DataFrame(augmented_X, columns=X.columns)\n",
    "    augmented_y = pd.Series(augmented_y)\n",
    "\n",
    "    return augmented_X, augmented_y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data = pd.read_csv(data_root + '/raw_mnist_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pixel1</th>\n",
       "      <th>pixel2</th>\n",
       "      <th>pixel3</th>\n",
       "      <th>pixel4</th>\n",
       "      <th>pixel5</th>\n",
       "      <th>pixel6</th>\n",
       "      <th>pixel7</th>\n",
       "      <th>pixel8</th>\n",
       "      <th>pixel9</th>\n",
       "      <th>pixel10</th>\n",
       "      <th>...</th>\n",
       "      <th>pixel776</th>\n",
       "      <th>pixel777</th>\n",
       "      <th>pixel778</th>\n",
       "      <th>pixel779</th>\n",
       "      <th>pixel780</th>\n",
       "      <th>pixel781</th>\n",
       "      <th>pixel782</th>\n",
       "      <th>pixel783</th>\n",
       "      <th>pixel784</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 785 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   pixel1  pixel2  pixel3  pixel4  pixel5  pixel6  pixel7  pixel8  pixel9  \\\n",
       "0       0       0       0       0       0       0       0       0       0   \n",
       "1       0       0       0       0       0       0       0       0       0   \n",
       "2       0       0       0       0       0       0       0       0       0   \n",
       "3       0       0       0       0       0       0       0       0       0   \n",
       "4       0       0       0       0       0       0       0       0       0   \n",
       "\n",
       "   pixel10  ...  pixel776  pixel777  pixel778  pixel779  pixel780  pixel781  \\\n",
       "0        0  ...         0         0         0         0         0         0   \n",
       "1        0  ...         0         0         0         0         0         0   \n",
       "2        0  ...         0         0         0         0         0         0   \n",
       "3        0  ...         0         0         0         0         0         0   \n",
       "4        0  ...         0         0         0         0         0         0   \n",
       "\n",
       "   pixel782  pixel783  pixel784  class  \n",
       "0         0         0         0      5  \n",
       "1         0         0         0      0  \n",
       "2         0         0         0      4  \n",
       "3         0         0         0      1  \n",
       "4         0         0         0      9  \n",
       "\n",
       "[5 rows x 785 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(70000, 785)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train, Validation & Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "## split the data into variables and target\n",
    "X = raw_data.drop(columns=['class'])\n",
    "y = raw_data['class']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((44800, 784), (11200, 784), (14000, 784))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## split train, validate and test data splits\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42)\n",
    "\n",
    "X_train.shape, X_val.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Augment Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Augmenting Data: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 44800/44800 [00:01<00:00, 25133.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "converting lists to DataFrame and Series\n"
     ]
    }
   ],
   "source": [
    "augmented_X, augmented_y = augment_dataset(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((224000, 784), (224000,))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "augmented_X.shape, augmented_y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "## save the augmented data\n",
    "augmented_data = pd.concat([augmented_X, augmented_y], axis=1)\n",
    "augmented_data.to_csv(data_root + '/augmented_ensemle_learning_mnist_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "## read the augmented data\n",
    "augmented_data = pd.read_csv(data_root + '/augmented_ensemle_learning_mnist_data.csv')\n",
    "\n",
    "## split the data into variables and target\n",
    "augmented_X = augmented_data.drop(columns=['0'])\n",
    "augmented_y = augmented_data['0']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Models using default params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dir = project_root + '/models/ensemble/'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# initialize LogisticRegression\n",
    "logistic_regression = LogisticRegression(n_jobs=-1, random_state=42, max_iter=10000)\n",
    "\n",
    "# create pipeline\n",
    "logistic_regression_pipeline = Pipeline([\n",
    "    (\"scaler\", MinMaxScaler()),\n",
    "    (\"logistic_regression\", logistic_regression)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time: 288.37s\n"
     ]
    }
   ],
   "source": [
    "## fit the model\n",
    "start_time = time.time()\n",
    "logistic_regression_pipeline.fit(augmented_X, augmented_y)\n",
    "end_time = time.time()\n",
    "print(f\"Training time: {end_time - start_time:.2f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "## save the model\n",
    "saved_location = joblib.dump(logistic_regression_pipeline, model_dir + 'logistic_regression_model.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "svc_base = SVC(probability=True, random_state=42)  # Enable probability for AUC\n",
    "\n",
    "# create pipeline\n",
    "default_svc_pipeline = Pipeline([\n",
    "    (\"scaler\", MinMaxScaler()),\n",
    "    (\"svc\", svc_base)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time: 12811.00s\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "default_svc_pipeline.fit(augmented_X, augmented_y)\n",
    "end_time = time.time()\n",
    "print(f\"Training time: {end_time - start_time:.2f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "## save the model\n",
    "saved_location = joblib.dump(default_svc_pipeline, model_dir + 'svc_model.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfc = RandomForestClassifier(random_state=42, n_jobs=-1)\n",
    "\n",
    "# Create a pipeline\n",
    "rfc_pipeline = Pipeline([\n",
    "    (\"scaler\", MinMaxScaler()),\n",
    "    (\"randomforest\", rfc)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time: 21.82s\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "rfc_pipeline.fit(augmented_X, augmented_y)\n",
    "end_time = time.time()\n",
    "print(f\"Training time: {end_time - start_time:.2f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "## save the model\n",
    "saved_location = joblib.dump(rfc_pipeline, model_dir + 'random_forest_model.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn = KNeighborsClassifier( n_jobs=-1)\n",
    "\n",
    "# Create a pipeline\n",
    "knn_pipeline = Pipeline([\n",
    "    (\"scaler\", MinMaxScaler()),\n",
    "    (\"knn\", knn)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time: 2.38s\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "knn_pipeline.fit(augmented_X, augmented_y)\n",
    "end_time = time.time()\n",
    "print(f\"Training time: {end_time - start_time:.2f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "## save the model\n",
    "saved_location = joblib.dump(knn_pipeline, model_dir + 'knn_model.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extra Trees Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time: 28.31s\n"
     ]
    }
   ],
   "source": [
    "extra_trees = ExtraTreesClassifier(n_jobs=-1, random_state=42)\n",
    "\n",
    "# Create a pipeline\n",
    "extra_trees_pipeline = Pipeline([\n",
    "    (\"scaler\", MinMaxScaler()),\n",
    "    (\"extra_trees\", extra_trees)\n",
    "])\n",
    "\n",
    "start_time = time.time()\n",
    "extra_trees_pipeline.fit(augmented_X, augmented_y)\n",
    "end_time = time.time()\n",
    "print(f\"Training time: {end_time - start_time:.2f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "## save the model\n",
    "saved_location = joblib.dump(extra_trees_pipeline, model_dir + 'extra_trees_model.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Boosting Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time: 6551.28s\n"
     ]
    }
   ],
   "source": [
    "## Gradient Boosting Classifier\n",
    "gb = GradientBoostingClassifier(n_estimators=100, random_state=42)\n",
    "\n",
    "# Create a pipeline\n",
    "gb_pipeline = Pipeline([\n",
    "    (\"scaler\", MinMaxScaler()),\n",
    "    (\"gb\", gb)\n",
    "])\n",
    "\n",
    "start_time = time.time()\n",
    "gb_pipeline.fit(augmented_X, augmented_y)\n",
    "end_time = time.time()\n",
    "print(f\"Training time: {end_time - start_time:.2f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "## save the model\n",
    "saved_location = joblib.dump(gb_pipeline, model_dir + 'gradient_boosting_model.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ensemble with Voting Classifier (Hard Voting)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## read all saved details in case the notebook is restarted\n",
    "model_dir = project_root + '/models/ensemble/'\n",
    "data_dir = project_root + '/data/'\n",
    "\n",
    "## read the augmented data from augmented_ensemle_learning_mnist_data.csv\n",
    "augmented_data = pd.read_csv(data_dir + 'augmented_ensemle_learning_mnist_data.csv')\n",
    "\n",
    "## split the data into variables and target\n",
    "augmented_X = augmented_data.drop(columns=['0'])\n",
    "augmented_y = augmented_data['0']\n",
    "\n",
    "\n",
    "## read estimators from saved models\n",
    "logistic_regression = joblib.load(model_dir + 'logistic_regression_model.pkl')\n",
    "svc = joblib.load(model_dir + 'svc_model.pkl')\n",
    "random_forest = joblib.load(model_dir + 'random_forest_model.pkl')\n",
    "knn = joblib.load(model_dir + 'knn_model.pkl')\n",
    "extra_trees = joblib.load(model_dir + 'extra_trees_model.pkl')\n",
    "gradient_boosting = joblib.load(model_dir + 'gradient_boosting_model.pkl')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "voting_clf = VotingClassifier(\n",
    "    estimators=[\n",
    "        (\"LogReg\", logistic_regression),\n",
    "        (\"SVC\", svc),\n",
    "        (\"RF\", random_forest),\n",
    "        (\"KNN\", knn),\n",
    "        (\"ExtraTrees\", extra_trees),\n",
    "        (\"GB\", gradient_boosting)\n",
    "    ],\n",
    "    voting=\"hard\"  # Use \"soft\" if all models support probability predictions\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* fit (Optional but Necessary for scikit-learn VotingClassifier): While the models are pre-trained, the VotingClassifier still needs to be \"fitted\" to understand the labels (classes). \n",
    "* We can fit it with a small dummy dataset, or an empty dataset, or even just pass an empty array for X and y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time: 2108.68s\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "voting_clf.fit(X_train, y_train)\n",
    "end_time = time.time()\n",
    "print(f\"Training time: {end_time - start_time:.2f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "## save the model\n",
    "saved_location = joblib.dump(voting_clf, model_dir + 'voting_classifier_model.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "## predict on the validation set\n",
    "y_val_pred = voting_clf.predict(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute metrics\n",
    "accuracy = accuracy_score(y_val, y_val_pred)\n",
    "weighted_f1 = f1_score(y_val, y_val_pred, average='weighted')\n",
    "# Compute per-class F1 scores\n",
    "per_class_f1_scores = f1_score(y_val, y_val_pred, average=None)\n",
    "per_class_f1_dict = {f\"Class_{i}\": score for i, score in enumerate(per_class_f1_scores)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9721428571428572\n",
      "Weighted F1: 0.9720921798301937\n",
      "Per-class F1 scores:  {'Class_0': 0.9764192139737992, 'Class_1': 0.9886851346078814, 'Class_2': 0.9684400360685302, 'Class_3': 0.9691438504997827, 'Class_4': 0.9760506100316313, 'Class_5': 0.9781659388646288, 'Class_6': 0.9752747252747253, 'Class_7': 0.9704090513489991, 'Class_8': 0.9644341801385681, 'Class_9': 0.9511153298528714}\n"
     ]
    }
   ],
   "source": [
    "## print the metrics\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(f\"Weighted F1: {weighted_f1}\")\n",
    "print(\"Per-class F1 scores: \", per_class_f1_dict)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ensemble with Voting Classifier (Soft Voting)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observations:\n",
    "* So the `Accuracy` and `F1 Scores`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "soft_voting_clf = VotingClassifier(\n",
    "    estimators=[\n",
    "        (\"LogReg\", logistic_regression),\n",
    "        (\"SVC\", svc),\n",
    "        (\"RF\", random_forest),\n",
    "        (\"KNN\", knn),\n",
    "        (\"ExtraTrees\", extra_trees),\n",
    "        (\"GB\", gradient_boosting)\n",
    "    ],\n",
    "    voting=\"soft\"  # Use \"soft\" if all models support probability predictions\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time: 2094.59s\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "soft_voting_clf.fit(X_train, y_train)\n",
    "end_time = time.time()\n",
    "print(f\"Training time: {end_time - start_time:.2f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "## save the model\n",
    "saved_location = joblib.dump(soft_voting_clf, model_dir + 'soft_voting_classifier_model.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "## predict on the validation set\n",
    "y_val_pred = soft_voting_clf.predict(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute metrics\n",
    "accuracy = accuracy_score(y_val, y_val_pred)\n",
    "weighted_f1 = f1_score(y_val, y_val_pred, average='weighted')\n",
    "# Compute per-class F1 scores\n",
    "per_class_f1_scores = f1_score(y_val, y_val_pred, average=None)\n",
    "per_class_f1_dict = {f\"Class_{i}\": score for i, score in enumerate(per_class_f1_scores)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9729464285714285\n",
      "Weighted F1: 0.9729077740577333\n",
      "Per-class F1 scores:  {'Class_0': 0.9811155028546333, 'Class_1': 0.9852140077821012, 'Class_2': 0.9712984054669704, 'Class_3': 0.9698558322411533, 'Class_4': 0.9759637188208616, 'Class_5': 0.9744084983099952, 'Class_6': 0.9744292237442922, 'Class_7': 0.9722463139635733, 'Class_8': 0.9677716390423573, 'Class_9': 0.9543529411764706}\n"
     ]
    }
   ],
   "source": [
    "## print the metrics\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(f\"Weighted F1: {weighted_f1}\")\n",
    "print(\"Per-class F1 scores: \", per_class_f1_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observations:\n",
    "* So voting classifier significantly outperforms most of the individual default estimators.\n",
    "* The closest model was default `SVC`, but its still not better then tuned `SVM` that we ended up using in prod, which had accuract and F1 score of .9897. \n",
    "* Next step would be to tune all the models and create an ensemble using the best model.\n",
    "    * For that we'll tune all the models using GridSearch except SVC, since we already have best params for that and training is time consuming. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyper-param Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* In order to save time, on training models and focusing on learning, we are going to train the models using the best params that we found in previous work sheet. \n",
    "* Even though the best params were found on non-augmented data, they are likely to perform well on augmented data too.\n",
    "* We can add hyper-param tuning as future enhancements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Below are the hyperparameters for the models\n",
    "#  Preprocessing: `normalize`\n",
    "#  Solver : `newton-cg`\n",
    "#  C : `0.1`\n",
    "#  Penalty : `l2`\n",
    "# initialize LogisticRegression\n",
    "logistic_regression = LogisticRegression(solver=\"newton-cg\", C=0.1, penalty=\"l2\", n_jobs=-1, random_state=42, max_iter=10000)\n",
    "\n",
    "# create pipeline\n",
    "tuned_logistic_regression_pipeline = Pipeline([\n",
    "    (\"scaler\", MinMaxScaler()),\n",
    "    (\"logisticregression\", logistic_regression)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time: 131.65s\n"
     ]
    }
   ],
   "source": [
    "## start timer\n",
    "\n",
    "start = time.time()\n",
    "tuned_logistic_regression_pipeline.fit(augmented_X, augmented_y)\n",
    "end = time.time()\n",
    "print(f\"Training time: {end - start:.2f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "## save the model\n",
    "saved_location = joblib.dump(tuned_logistic_regression_pipeline, model_dir + 'tuned_logistic_regression_model.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9197321428571429\n",
      "Weighted F1: 0.9194301831211195\n",
      "Per-class F1 scores:  {'Class_0': 0.9606986899563319, 'Class_1': 0.9517660463349791, 'Class_2': 0.9100185528756958, 'Class_3': 0.9082853345148427, 'Class_4': 0.9278996865203761, 'Class_5': 0.8815212091662603, 'Class_6': 0.9409610983981693, 'Class_7': 0.9341527655838455, 'Class_8': 0.872794800371402, 'Class_9': 0.8969024503005085}\n"
     ]
    }
   ],
   "source": [
    "## predict on the validation set\n",
    "y_val_pred = tuned_logistic_regression_pipeline.predict(X_val)\n",
    "# Compute metrics\n",
    "accuracy = accuracy_score(y_val, y_val_pred)\n",
    "weighted_f1 = f1_score(y_val, y_val_pred, average='weighted')\n",
    "# Compute per-class F1 scores\n",
    "per_class_f1_scores = f1_score(y_val, y_val_pred, average=None)\n",
    "per_class_f1_dict = {f\"Class_{i}\": score for i, score in enumerate(per_class_f1_scores)}\n",
    "## print the metrics\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(f\"Weighted F1: {weighted_f1}\")\n",
    "print(\"Per-class F1 scores: \", per_class_f1_dict)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating pipeline...\n"
     ]
    }
   ],
   "source": [
    "## training svc on the augmented data using best hyperparameters\n",
    "svc = SVC(C=10, gamma=\"scale\", kernel=\"rbf\", random_state=42, probability=True)\n",
    "\n",
    "# create pipeline\n",
    "\n",
    "print(\"Creating pipeline...\")\n",
    "svc_pipeline = Pipeline([\n",
    "    (\"scaler\", MinMaxScaler()),\n",
    "    (\"svc\", svc)\n",
    "], verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Pipeline] ............ (step 1 of 2) Processing scaler, total=   1.1s\n",
      "[Pipeline] .............. (step 2 of 2) Processing svc, total=250.8min\n",
      "Training time: 15048.24s\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "svc_pipeline.fit(augmented_X, augmented_y.values.ravel())\n",
    "end_time = time.time()\n",
    "print(f\"Training time: {end_time - start_time:.2f}s\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "## save the model\n",
    "saved_location = joblib.dump(svc_pipeline, model_dir + 'tuned_svc_model.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.98875\n",
      "Weighted F1: 0.9887464218354948\n",
      "Per-class F1 scores:  {'Class_0': 0.9911347517730497, 'Class_1': 0.9949159170903402, 'Class_2': 0.9882459312839059, 'Class_3': 0.9855579868708971, 'Class_4': 0.9873303167420815, 'Class_5': 0.9889049686444766, 'Class_6': 0.9876543209876543, 'Class_7': 0.9886759581881533, 'Class_8': 0.9876316994961063, 'Class_9': 0.9864549276039234}\n"
     ]
    }
   ],
   "source": [
    "## predict on the validation set\n",
    "y_val_pred = svc_pipeline.predict(X_val)\n",
    "# Compute metrics\n",
    "accuracy = accuracy_score(y_val, y_val_pred)\n",
    "weighted_f1 = f1_score(y_val, y_val_pred, average='weighted')\n",
    "# Compute per-class F1 scores\n",
    "per_class_f1_scores = f1_score(y_val, y_val_pred, average=None)\n",
    "per_class_f1_dict = {f\"Class_{i}\": score for i, score in enumerate(per_class_f1_scores)}\n",
    "## print the metrics\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(f\"Weighted F1: {weighted_f1}\")\n",
    "print(\"Per-class F1 scores: \", per_class_f1_dict)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time: 35.12s\n",
      "Accuracy: 0.9798214285714286\n",
      "Weighted F1: 0.9798328131009487\n",
      "Per-class F1 scores:  {'Class_0': 0.9850088183421517, 'Class_1': 0.9901768172888016, 'Class_2': 0.9805517865219358, 'Class_3': 0.9781659388646288, 'Class_4': 0.9808743169398907, 'Class_5': 0.9845111326234269, 'Class_6': 0.9790145985401459, 'Class_7': 0.981230903535574, 'Class_8': 0.9743824336688015, 'Class_9': 0.9624129930394432}\n"
     ]
    }
   ],
   "source": [
    "## below are the best hyperparameters for the random forest classifier\n",
    "## Best Parameters: {'preprocessing__kw_args': {'method': 'normalize'}, 'randomforest__max_depth': None, 'randomforest__min_samples_leaf': 1, 'randomforest__min_samples_split': 2, 'randomforest__n_estimators': 200}\n",
    "\n",
    "rfc = RandomForestClassifier(max_depth=None, min_samples_leaf=1, min_samples_split=2, n_estimators=200, random_state=42, n_jobs=-1)\n",
    "\n",
    "tuned_rfc_pipeline = Pipeline([\n",
    "    (\"scaler\", MinMaxScaler()),\n",
    "    (\"randomforest\", rfc)\n",
    "])\n",
    "\n",
    "start = time.time()\n",
    "tuned_rfc_pipeline.fit(augmented_X, augmented_y)\n",
    "end = time.time()\n",
    "print(f\"Training time: {end - start:.2f}s\")\n",
    "\n",
    "## save the model\n",
    "saved_location = joblib.dump(tuned_rfc_pipeline, model_dir + 'tuned_random_forest_model.pkl')\n",
    "\n",
    "## predict on the validation set\n",
    "y_val_pred = tuned_rfc_pipeline.predict(X_val)\n",
    "# Compute metrics\n",
    "accuracy = accuracy_score(y_val, y_val_pred)\n",
    "weighted_f1 = f1_score(y_val, y_val_pred, average='weighted')\n",
    "# Compute per-class F1 scores\n",
    "per_class_f1_scores = f1_score(y_val, y_val_pred, average=None)\n",
    "per_class_f1_dict = {f\"Class_{i}\": score for i, score in enumerate(per_class_f1_scores)}\n",
    "## print the metrics\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(f\"Weighted F1: {weighted_f1}\")\n",
    "print(\"Per-class F1 scores: \", per_class_f1_dict)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time: 3.10s\n",
      "Accuracy: 0.97875\n",
      "Weighted F1: 0.9787129524087814\n",
      "Per-class F1 scores:  {'Class_0': 0.9846153846153847, 'Class_1': 0.9841637697952877, 'Class_2': 0.9803383630544125, 'Class_3': 0.9812472743131269, 'Class_4': 0.9803921568627451, 'Class_5': 0.9806949806949807, 'Class_6': 0.9809264305177112, 'Class_7': 0.9813449023861172, 'Class_8': 0.9661971830985916, 'Class_9': 0.9656453110492108}\n"
     ]
    }
   ],
   "source": [
    "## Best KNN Params are \n",
    "## {'knn__algorithm': 'auto', 'knn__n_neighbors': 5, 'knn__p': 2, 'knn__weights': 'distance'}\n",
    "\n",
    "knn = KNeighborsClassifier(algorithm='auto', n_neighbors=5, p=2, weights='distance', n_jobs=-1)\n",
    "\n",
    "tuned_knn_pipeline = Pipeline([\n",
    "    (\"scaler\", MinMaxScaler()),\n",
    "    (\"knn\", knn)\n",
    "])\n",
    "\n",
    "start = time.time()\n",
    "tuned_knn_pipeline.fit(augmented_X, augmented_y)\n",
    "end = time.time()\n",
    "\n",
    "print(f\"Training time: {end - start:.2f}s\")\n",
    "## save the model\n",
    "saved_location = joblib.dump(tuned_knn_pipeline, model_dir + 'tuned_knn_model.pkl')\n",
    "\n",
    "## predict on the validation set\n",
    "y_val_pred = tuned_knn_pipeline.predict(X_val)\n",
    "# Compute metrics\n",
    "accuracy = accuracy_score(y_val, y_val_pred)\n",
    "weighted_f1 = f1_score(y_val, y_val_pred, average='weighted')\n",
    "# Compute per-class F1 scores\n",
    "per_class_f1_scores = f1_score(y_val, y_val_pred, average=None)\n",
    "per_class_f1_dict = {f\"Class_{i}\": score for i, score in enumerate(per_class_f1_scores)}\n",
    "## print the metrics\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(f\"Weighted F1: {weighted_f1}\")\n",
    "print(\"Per-class F1 scores: \", per_class_f1_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extra Trees Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* We don't have best params for `Extra Trees Classifier` so we'll need to run grid search for this. \n",
    "* We'll try to see if we can fit and find the best params quickly else we'll skip this model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 216 candidates, totalling 648 fits\n",
      "Training time: 13793.25s\n"
     ]
    }
   ],
   "source": [
    "param_grid = {\n",
    "    \"extra_trees__n_estimators\": [100, 200, 300],\n",
    "    \"extra_trees__max_depth\": [None, 10, 20, 30],\n",
    "    \"extra_trees__min_samples_split\": [2, 5, 10],\n",
    "    \"extra_trees__min_samples_leaf\": [1, 2, 4],\n",
    "    \"extra_trees__bootstrap\": [True, False]\n",
    "}\n",
    "\n",
    "extra_trees = ExtraTreesClassifier(n_jobs=-1, random_state=42)\n",
    "\n",
    "# Create a pipeline\n",
    "extra_trees_pipeline = Pipeline([\n",
    "    (\"scaler\", MinMaxScaler()),\n",
    "    (\"extra_trees\", extra_trees)\n",
    "])\n",
    "\n",
    "## important thing to notice here, setting n_jobs=-1 in RandomizedSearchCV will cause the notebook to crash.\n",
    "extra_trees_grid_search = GridSearchCV(extra_trees_pipeline, param_grid, cv=3, verbose=1 , n_jobs=4)\n",
    "\n",
    "\n",
    "start = time.time()\n",
    "extra_trees_grid_search.fit(augmented_X, augmented_y)\n",
    "end = time.time()\n",
    "print(f\"Training time: {end - start:.2f}s\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Just a interesting note, all the time invested in setting up jupyter server on a gaming machine and moving the training there, paid off. The above grid search took ~4 hours of training, but previously it didn't finish training for 12+ hours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters:  {'extra_trees__bootstrap': False, 'extra_trees__max_depth': None, 'extra_trees__min_samples_leaf': 1, 'extra_trees__min_samples_split': 2, 'extra_trees__n_estimators': 300}\n",
      "Best Score:  0.9723169649422987\n"
     ]
    }
   ],
   "source": [
    "## print the best parameters\n",
    "print(\"Best Parameters: \", extra_trees_grid_search.best_params_)\n",
    "## print the best score\n",
    "print(\"Best Score: \", extra_trees_grid_search.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "## save the model\n",
    "saved_location = joblib.dump(extra_trees_grid_search.best_estimator_, model_dir + 'tuned_extra_trees_model.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('ensemble/tuned_extra_trees_model.pkl')"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Path(model_dir + 'tuned_extra_trees_model.pkl').relative_to(model_root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "82b99367a7714e94ab8f73406a82774e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tuned_extra_trees_model.pkl:   0%|          | 0.00/3.05G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/gaurangdave/mnist_digits_recognition/commit/7c8a68208b45637713d7151c7a03f704b02137b7', commit_message='Upload ensemble/tuned_extra_trees_model.pkl with huggingface_hub', commit_description='', oid='7c8a68208b45637713d7151c7a03f704b02137b7', pr_url=None, repo_url=RepoUrl('https://huggingface.co/gaurangdave/mnist_digits_recognition', endpoint='https://huggingface.co', repo_type='model', repo_id='gaurangdave/mnist_digits_recognition'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## upload the model to hugging face\n",
    "path_or_fileobj = Path(model_dir + 'tuned_extra_trees_model.pkl')\n",
    "path_in_repo = path_or_fileobj.relative_to(model_root)\n",
    "api.upload_file(repo_id=repo_id, path_or_fileobj=path_or_fileobj,path_in_repo=str(path_in_repo), token=HF_TOKEN, repo_type=\"model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9810714285714286\n",
      "Weighted F1: 0.9810816413993403\n",
      "Per-class F1 scores:  {'Class_0': 0.9876434245366285, 'Class_1': 0.9913657770800628, 'Class_2': 0.9791099000908265, 'Class_3': 0.9773123909249564, 'Class_4': 0.9836214740673339, 'Class_5': 0.9845261121856866, 'Class_6': 0.98359161349134, 'Class_7': 0.9825174825174825, 'Class_8': 0.9739368998628258, 'Class_9': 0.9652294853963839}\n"
     ]
    }
   ],
   "source": [
    "## predict on the validation set\n",
    "y_val_pred = extra_trees_grid_search.best_estimator_.predict(X_val)\n",
    "# Compute metrics\n",
    "accuracy = accuracy_score(y_val, y_val_pred)\n",
    "weighted_f1 = f1_score(y_val, y_val_pred, average='weighted')\n",
    "# Compute per-class F1 scores\n",
    "per_class_f1_scores = f1_score(y_val, y_val_pred, average=None)\n",
    "per_class_f1_dict = {f\"Class_{i}\": score for i, score in enumerate(per_class_f1_scores)}\n",
    "## print the metrics\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(f\"Weighted F1: {weighted_f1}\")\n",
    "print(\"Per-class F1 scores: \", per_class_f1_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Boosting Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 20 candidates, totalling 60 fits\n",
      "Training time: 25558.94s\n"
     ]
    }
   ],
   "source": [
    "param_grid = {\n",
    "    \"n_estimators\": [100, 200, 300],\n",
    "    \"learning_rate\": [0.01, 0.1, 1],\n",
    "    \"max_depth\": [3, 5, 7],\n",
    "    \"subsample\": [0.8, 0.9, 1.0],\n",
    "    \"max_features\": [None, \"sqrt\", \"log2\"]\n",
    "}\n",
    "\n",
    "gb_clf = GradientBoostingClassifier(random_state=42)\n",
    "\n",
    "# Create a pipeline\n",
    "# gb_pipeline = Pipeline([\n",
    "#     (\"scaler\", MinMaxScaler()),\n",
    "#     (\"gb\", gb_clf)\n",
    "# ])\n",
    "\n",
    "gb_grid_search = RandomizedSearchCV(gb_clf, param_grid, cv=3, verbose=1,  n_jobs=10, n_iter=20,random_state=42)\n",
    "\n",
    "start = time.time()\n",
    "gb_grid_search.fit(augmented_X, augmented_y)\n",
    "end = time.time()\n",
    "print(f\"Training time: {end - start:.2f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters:  {'subsample': 1.0, 'n_estimators': 300, 'max_features': 'sqrt', 'max_depth': 7, 'learning_rate': 0.1}\n",
      "Best Score:  0.9789017826243046\n"
     ]
    }
   ],
   "source": [
    "## print the best parameters\n",
    "print(\"Best Parameters: \", gb_grid_search.best_params_)\n",
    "## print the best score\n",
    "print(\"Best Score: \", gb_grid_search.best_score_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "## save the model\n",
    "saved_location = joblib.dump(gb_grid_search.best_estimator_, model_dir + 'tuned_gradient_boosting_model.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7198a4791f0e4f71a4fd108effb27560",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tuned_gradient_boosting_model.pkl:   0%|          | 0.00/66.6M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/gaurangdave/mnist_digits_recognition/commit/b7cdd390f3d2ae7719a6e803250d3758b67927c9', commit_message='Upload ensemble/tuned_gradient_boosting_model.pkl with huggingface_hub', commit_description='', oid='b7cdd390f3d2ae7719a6e803250d3758b67927c9', pr_url=None, repo_url=RepoUrl('https://huggingface.co/gaurangdave/mnist_digits_recognition', endpoint='https://huggingface.co', repo_type='model', repo_id='gaurangdave/mnist_digits_recognition'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## upload the model to hugging face\n",
    "path_or_fileobj = Path(model_dir + 'tuned_gradient_boosting_model.pkl')\n",
    "path_in_repo = path_or_fileobj.relative_to(model_root)\n",
    "api.upload_file(repo_id=repo_id, path_or_fileobj=path_or_fileobj,path_in_repo=str(path_in_repo), token=HF_TOKEN, repo_type=\"model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9860714285714286\n",
      "Weighted F1: 0.9860734485464957\n",
      "Per-class F1 scores:  {'Class_0': 0.9876325088339223, 'Class_1': 0.9956744003145891, 'Class_2': 0.9823449524671797, 'Class_3': 0.9846827133479212, 'Class_4': 0.9855334538878843, 'Class_5': 0.9893822393822393, 'Class_6': 0.9835164835164835, 'Class_7': 0.9851657940663177, 'Class_8': 0.9836065573770492, 'Class_9': 0.9818012132524498}\n"
     ]
    }
   ],
   "source": [
    "## predict on the validation set\n",
    "y_val_pred = gb_grid_search.best_estimator_.predict(X_val)\n",
    "# Compute metrics\n",
    "accuracy = accuracy_score(y_val, y_val_pred)\n",
    "weighted_f1 = f1_score(y_val, y_val_pred, average='weighted')\n",
    "# Compute per-class F1 scores\n",
    "per_class_f1_scores = f1_score(y_val, y_val_pred, average=None)\n",
    "per_class_f1_dict = {f\"Class_{i}\": score for i, score in enumerate(per_class_f1_scores)}\n",
    "## print the metrics\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(f\"Weighted F1: {weighted_f1}\")\n",
    "print(\"Per-class F1 scores: \", per_class_f1_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
